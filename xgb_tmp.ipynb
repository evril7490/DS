{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efea1d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwzhu/anaconda3/envs/jlib38/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "# import pandas for data wrangling\n",
    "import pandas as pd\n",
    "\n",
    "import lightgbm\n",
    "\n",
    "# import numpy for Scientific computations\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# import machine learning libraries\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# import packages for hyperparameters tuning\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bd451e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#.choice(label, options) — Returns one of the options, which should be a list or tuple.\n",
    "\n",
    "\n",
    "#hp.quniform(label, low, high, q) — Returns a value round(uniform(low, high) / q) * q, i.e it rounds the decimal values and returns an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c5763a",
   "metadata": {},
   "outputs": [],
   "source": [
    "space={'max_depth': hp.quniform(\"max_depth\", 3, 18, 1),\n",
    "        'gamma': hp.uniform ('gamma', 1,9),\n",
    "        'reg_alpha' : hp.quniform('reg_alpha', 40,180,1),\n",
    "        'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n",
    "        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n",
    "        'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n",
    "        'n_estimators': 180,\n",
    "        'seed': 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "423845ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(space):\n",
    "    clf=xgb.XGBRegressor(\n",
    "                    objective='reg:squarederror'\n",
    "                    n_estimators =space['n_estimators'], \n",
    "                    max_depth = int(space['max_depth']), \n",
    "                    gamma = space['gamma'],\n",
    "                    reg_alpha = int(space['reg_alpha']),\n",
    "                    min_child_weight=int(space['min_child_weight']),\n",
    "                    colsample_bytree=int(space['colsample_bytree'])\n",
    "                    )\n",
    "    \n",
    "    evaluation = [( X_train, y_train), ( X_test, y_test)]\n",
    "    \n",
    "    clf.fit(X_train, y_train,\n",
    "            eval_set=evaluation, eval_metric=\"auc\",\n",
    "            early_stopping_rounds=10,verbose=False)\n",
    "    \n",
    "\n",
    "    pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, pred>0.5)\n",
    "    print (\"SCORE:\", accuracy)\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cb00fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "\n",
    "best_hyperparams = fmin(fn = objective,\n",
    "                        space = space,\n",
    "                        algo = tpe.suggest,\n",
    "                        max_evals = 100,\n",
    "                        trials = trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9aa180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08504737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_asymmetric_train(y_true, y_pred):\n",
    "    residual = (y_true - y_pred).astype(\"float\")\n",
    "    grad = np.where(residual<0, -2*10.0*residual, -2*residual)\n",
    "    hess = np.where(residual<0, 2*10.0, 2.0)\n",
    "    return grad, hess\n",
    "\n",
    "def custom_asymmetric_valid(y_true, y_pred):\n",
    "    residual = (y_true - y_pred).astype(\"float\")\n",
    "    loss = np.where(residual < 0, (residual**2)*10.0, residual**2) \n",
    "    return \"custom_asymmetric_eval\", np.mean(loss), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195420a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/dmlc/xgboost/blob/master/demo/guide-python/custom_rmsle.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21f3bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://forum.numer.ai/t/custom-loss-functions-for-xgboost-using-pytorch/960/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f13cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://forum.numer.ai/t/custom-loss-functions-for-xgboost-using-pytorch/960"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe24b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using python closure\n",
    "def focal_loss(alpha, gamma):\n",
    "    def custom_loss(y_pred, y_true):\n",
    "        a,g = alpha, gamma\n",
    "        def fl(x,t):\n",
    "            p = 1/(1+np.exp(-x))\n",
    "            return -( a*t + (1-a)*(1-t) ) * (( 1 - ( t*p + (1-t)*(1-p)) )**g) * ( t*np.log(p)+(1-t)*np.log(1-p) )\n",
    "        partial_fl = lambda x: fl(x, y_true)\n",
    "        grad = derivative(partial_fl, y_pred, n=1, dx=1e-6)\n",
    "        hess = derivative(partial_fl, y_pred, n=2, dx=1e-6)\n",
    "        return grad, hess\n",
    "    return custom_loss\n",
    "\n",
    "xgb = xgb.XGBClassifier(objective=focal_loss(alpha=0.25, gamma=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11360982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8ea2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default lightgbm model with sklearn api\n",
    "gbm = lightgbm.LGBMRegressor() \n",
    "\n",
    "# updating objective function to custom\n",
    "# default is \"regression\"\n",
    "# also adding metrics to check different scores\n",
    "gbm.set_params(**{'objective': custom_asymmetric_train}, metrics = [\"mse\", 'mae'])\n",
    "\n",
    "# fitting model \n",
    "gbm.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=[(X_valid, y_valid)],\n",
    "    eval_metric=custom_asymmetric_valid,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "y_pred = gbm.predict(X_valid)\n",
    "\n",
    "# create dataset for lightgbm\n",
    "# if you want to re-use data, remember to set free_raw_data=False\n",
    "lgb_train = lgb.Dataset(X_train, y_train, free_raw_data=False)\n",
    "lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train, free_raw_data=False)\n",
    "\n",
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=10,\n",
    "                init_model=gbm,\n",
    "                fobj=custom_asymmetric_train,\n",
    "                feval=custom_asymmetric_valid,\n",
    "                valid_sets=lgb_eval)\n",
    "                \n",
    "y_pred = gbm.predict(X_valid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jlib38)",
   "language": "python",
   "name": "jlib38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
